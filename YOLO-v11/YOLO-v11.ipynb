{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936c91ad",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ train-3000.csv: 14122 rows, 50.0% positive\n",
      "→ test-3000.csv: 3532 rows, 50.0% positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/2jz1j73d6kl_wvvcg3ld5d_00000gn/T/ipykernel_1769/2351497194.py:34: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  test_neg       = set(random.sample(remaining_negs, len(test_pos)))\n"
     ]
    }
   ],
   "source": [
    "# If you don’t already have pandas installed, uncomment:\n",
    "# pip install pandas\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# ─── Config ────────────────────────────────────────────────────────────────────\n",
    "NUM_POS       = 8827\n",
    "NUM_NEG       = 1050302\n",
    "TRAIN_FRAC_POS = 0.8    # 80% of positives → train\n",
    "SEED          = 42      # reproducibility\n",
    "PAD_WIDTH     = 5       # zero‑pad numeric IDs to at least 5 digits\n",
    "\n",
    "# ─── Output paths ──────────────────────────────────────────────────────────────\n",
    "base_dir  = \"/Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data\"\n",
    "csv_train = os.path.join(base_dir, \"TestTrainSplits\", \"train_test_easy\", \"train-3000.csv\")\n",
    "csv_test  = os.path.join(base_dir, \"TestTrainSplits\", \"train_test_easy\", \"test-3000.csv\")\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "# ─── 1) Build zero‑padded name lists ───────────────────────────────────────────\n",
    "positives = [f\"P{i:0{PAD_WIDTH}d}\" for i in range(1, NUM_POS + 1)]\n",
    "negatives = [f\"N{i:0{7}d}\" for i in range(1, NUM_NEG + 1)]\n",
    "\n",
    "# ─── 2) 80/20 split on positives ───────────────────────────────────────────────\n",
    "num_train_pos = int(len(positives) * TRAIN_FRAC_POS)\n",
    "train_pos     = set(random.sample(positives, num_train_pos))\n",
    "test_pos      = set(positives) - train_pos\n",
    "\n",
    "# ─── 3) Sample negatives to get 50:50 balance ─────────────────────────────────\n",
    "train_neg      = set(random.sample(negatives, len(train_pos)))\n",
    "remaining_negs = set(negatives) - train_neg\n",
    "test_neg       = set(random.sample(remaining_negs, len(test_pos)))\n",
    "\n",
    "# ─── 4) Build, shuffle, and save splits ───────────────────────────────────────\n",
    "def save_split(pos_set, neg_set, out_path):\n",
    "    df = pd.DataFrame({\n",
    "        \"name\":       list(pos_set) + list(neg_set),\n",
    "        \"is_positive\": [True]*len(pos_set) + [False]*len(neg_set)\n",
    "    })\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    pct = df[\"is_positive\"].mean() * 100\n",
    "    print(f\"→ {os.path.basename(out_path)}: {len(df)} rows, {pct:.1f}% positive\")\n",
    "\n",
    "save_split(train_pos, train_neg, csv_train)\n",
    "save_split(test_pos,  test_neg,  csv_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images shape: torch.Size([32, 3, 416, 416])\n",
      "Train Targets: [tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([[2.0000, 0.6061, 0.5162, 0.1804, 0.4716]]), tensor([[3.0000, 0.5864, 0.4855, 0.0877, 0.2619]]), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([[2.0000, 0.8380, 0.5852, 0.0394, 0.1645],\n",
      "        [3.0000, 0.6074, 0.7167, 0.1677, 0.3187],\n",
      "        [3.0000, 0.9053, 0.5950, 0.0623, 0.2260]]), tensor([], size=(0, 5)), tensor([[0.0000, 0.1639, 0.2364, 0.2211, 0.3152],\n",
      "        [0.0000, 0.3596, 0.3482, 0.3837, 0.3395]]), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([[4.0000, 0.3488, 0.3760, 0.3393, 0.1935]]), tensor([[2.0000, 0.3342, 0.5139, 0.0584, 0.2329]]), tensor([[0.0000, 0.2484, 0.5377, 0.3748, 0.3013],\n",
      "        [1.0000, 0.3482, 0.4247, 0.1957, 0.6153],\n",
      "        [1.0000, 0.2662, 0.4531, 0.2402, 0.6257],\n",
      "        [2.0000, 0.4600, 0.6883, 0.6734, 0.6165],\n",
      "        [2.0000, 0.2141, 0.3592, 0.2351, 0.4936],\n",
      "        [4.0000, 0.4066, 0.1900, 0.4371, 0.2665]]), tensor([[1.0000, 0.7484, 0.1744, 0.2084, 0.1472],\n",
      "        [1.0000, 0.6836, 0.4241, 0.2414, 0.1669]]), tensor([], size=(0, 5)), tensor([[4.0000, 0.4047, 0.6790, 0.1766, 0.1321]]), tensor([[4.0000, 0.6798, 0.5319, 0.0889, 0.1970]]), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([[4.0000, 0.7198, 0.6709, 0.0877, 0.0973],\n",
      "        [3.0000, 0.7757, 0.5921, 0.1029, 0.1298]]), tensor([], size=(0, 5)), tensor([], size=(0, 5)), tensor([[0.0000, 0.6563, 0.9096, 0.3520, 0.1808],\n",
      "        [0.0000, 0.9123, 0.5759, 0.1728, 0.2572]]), tensor([[3.0000, 0.3151, 0.7399, 0.4142, 0.3812],\n",
      "        [3.0000, 0.3189, 0.7787, 0.4828, 0.1391]]), tensor([], size=(0, 5)), tensor([[1.0000, 0.5083, 0.5655, 0.3329, 0.5794]]), tensor([], size=(0, 5))]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import get_dataloader, custom_collate_fn\n",
    "\n",
    "# =============================================================================\n",
    "# Adjust these paths according to your folder structure.\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # For the train_test_easy split\n",
    "    base_dir = \"/Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data\"\n",
    "    \n",
    "    # Use the CSV files in the easy split folder:\n",
    "    csv_train = os.path.join(base_dir, \"TestTrainSplits\", \"train_test_easy\", \"train-3000.csv\")\n",
    "    csv_test  = os.path.join(base_dir, \"TestTrainSplits\", \"train_test_easy\", \"test-3000.csv\")\n",
    "    \n",
    "    # Directory containing JPEG images.\n",
    "    images_dir = os.path.join(base_dir, \"JPEGImageFull\", \"dataset\", \"JPEGImage\")\n",
    "    # Directory containing positive XML annotations.\n",
    "    annotations_dir = os.path.join(base_dir, \"positive-Annotation\")\n",
    "    \n",
    "    # DataLoaders for training and testing.\n",
    "    # Pass the custom collate function here:\n",
    "    train_loader = get_dataloader(csv_train, images_dir, annotations_dir, batch_size=32, train=True)\n",
    "    test_loader  = get_dataloader(csv_test, images_dir, annotations_dir, batch_size=32, train=False)\n",
    "    \n",
    "    # When creating the DataLoader inside get_dataloader, set the collate_fn parameter\n",
    "    \n",
    "    # For our testing, we can either modify get_dataloader() or wrap it here:\n",
    "    from torch.utils.data import DataLoader\n",
    "    # Reconstruct using our custom_collate_fn for demonstration:\n",
    "    train_loader = DataLoader(train_loader.dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    # Simple test: iterate through one batch.\n",
    "    for imgs, targets in train_loader:\n",
    "        print(\"Train Images shape:\", imgs.shape)  # Expected: [batch, 3, 416, 416]\n",
    "        print(\"Train Targets:\", targets)  # A list, each element a tensor of shape [N, 4] (or [N, 5] if you include classes)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc9a21",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone Feature Shape: torch.Size([1, 40, 52, 52])\n",
      "Bounding Box Output Shape: torch.Size([1, 52, 52, 4])\n",
      "Objectness Output Shape: torch.Size([1, 52, 52, 1])\n",
      "Class Output Shape: torch.Size([1, 52, 52, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class SimpleDetectionHead(nn.Module):\n",
    "    def __init__(self, backbone_channels, num_classes=6, num_anchors=1): # Simplified: num_anchors=1 for simplicity\n",
    "        super(SimpleDetectionHead, self).__init__()\n",
    "        # Reduce channels from backbone\n",
    "        self.reduce_conv = nn.Conv2d(backbone_channels, 256, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Bounding box regression - 4 outputs (dx, dy, dw, dh)\n",
    "        self.bbox_regressor = nn.Conv2d(256, num_anchors * 4, kernel_size=3, padding=1)\n",
    "        # Objectness confidence - 1 output (probability)\n",
    "        self.objectness_classifier = nn.Conv2d(256, num_anchors * 1, kernel_size=3, padding=1)\n",
    "        # Class classification - num_classes outputs\n",
    "        self.class_classifier = nn.Conv2d(256, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = self.reduce_conv(features)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        bbox_output = self.bbox_regressor(x) # [B, 4, H, W]\n",
    "        objectness_output = self.objectness_classifier(x) # [B, 1, H, W]\n",
    "        class_output = self.class_classifier(x) # [B, num_classes, H, W]\n",
    "\n",
    "        # Reshape for easier handling later, removing anchor dimension if num_anchors=1\n",
    "        bbox_output = bbox_output.permute(0, 2, 3, 1).contiguous() # [B, H, W, 4]\n",
    "        objectness_output = objectness_output.permute(0, 2, 3, 1).contiguous() # [B, H, W, 1]\n",
    "        class_output = class_output.permute(0, 2, 3, 1).contiguous() # [B, H, W, num_classes]\n",
    "\n",
    "        return bbox_output, objectness_output, class_output\n",
    "\n",
    "\n",
    "class EfficientNetB0Detector(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(EfficientNetB0Detector, self).__init__()\n",
    "        # Load EfficientNet-B0 backbone, pretrained, features only, no classifier head\n",
    "        self.backbone = timm.create_model('efficientnet_b0', pretrained=True, features_only=True, out_indices=(2,)) # Using out_indices=(2,) to get a feature map of reasonable size\n",
    "        backbone_channels = self.backbone.feature_info.channels()[-1] # Get output channels of the selected feature level (level 2 in this case for efficientnet_b0)\n",
    "        self.detection_head = SimpleDetectionHead(backbone_channels, num_classes=num_classes)\n",
    "\n",
    "        # Freeze backbone weights (optional for starter model)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_features = self.backbone(x)[0] # Get feature map at index 2\n",
    "        bbox_pred, objectness_pred, class_pred = self.detection_head(backbone_features)\n",
    "        return bbox_pred, objectness_pred, class_pred # Return as separate outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Simple test\n",
    "    model = EfficientNetB0Detector(num_classes=6)\n",
    "    model.eval() # Set to eval mode for inference\n",
    "\n",
    "    input_tensor = torch.randn(1, 3, 416, 416) # Batch size 1, 3 channels, 416x416 image\n",
    "    bbox_output, objectness_output, class_output = model(input_tensor)\n",
    "\n",
    "    print(\"Backbone Feature Shape:\", model.backbone(input_tensor)[0].shape) # Check backbone feature shape\n",
    "    print(\"Bounding Box Output Shape:\", bbox_output.shape)      # Expected: [1, H, W, 4]\n",
    "    print(\"Objectness Output Shape:\", objectness_output.shape)  # Expected: [1, H, W, 1]\n",
    "    print(\"Class Output Shape:\", class_output.shape)         # Expected: [1, H, W, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting XML annotations to YOLO .txt format (skipping malformed objects)...\n",
      "Generating splits, creating missing .txt labels, and writing data.yaml…\n",
      "Writing data.yaml…\n",
      "Wrote data.yaml without a `path:`. YOLOv11 will now use your split files directly.\n",
      "STEP 2 complete.\n",
      "Loading YOLOv11 model (yolo11l.pt)...\n",
      "Training on device: cpu\n",
      "Phase 1: freezing backbone, training head...\n",
      "New https://pypi.org/project/ultralytics/8.3.119 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.105 🚀 Python-3.10.16 torch-2.4.1.post2 CPU (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11l.pt, data=/Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/data.yaml, epochs=2, time=None, patience=100, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=runs/yolo11, name=phase1, exist_ok=True, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/yolo11/phase1\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1415650  ultralytics.nn.modules.head.Detect           [6, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,315,106 parameters, 25,315,090 gradients\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/yolo11/phase1', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage... 14122 images, 7078 backgrounds, 0 corrupt: 100%|██████████| 14122/14122 [00:01<00:00, 7293.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage... 3532 images, 1770 backgrounds, 0 corrupt: 100%|██████████| 3532/3532 [00:00<00:00, 5739.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage.cache\n",
      "Plotting labels to runs/yolo11/phase1/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/yolo11/phase1\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2         0G      1.993      2.242      1.903          8        416: 100%|██████████| 883/883 [2:40:03<00:00, 10.88s/it]  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 111/111 [15:10<00:00,  8.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513       0.43      0.373      0.327      0.135\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2         0G      1.808      1.782      1.805          6        416: 100%|██████████| 883/883 [2:39:02<00:00, 10.81s/it]  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 111/111 [15:05<00:00,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513      0.612       0.48      0.521      0.258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 5.827 hours.\n",
      "Optimizer stripped from runs/yolo11/phase1/weights/last.pt, 51.2MB\n",
      "Optimizer stripped from runs/yolo11/phase1/weights/best.pt, 51.2MB\n",
      "\n",
      "Validating runs/yolo11/phase1/weights/best.pt...\n",
      "Ultralytics 8.3.105 🚀 Python-3.10.16 torch-2.4.1.post2 CPU (Apple M4)\n",
      "YOLO11l summary (fused): 190 layers, 25,283,938 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 111/111 [14:30<00:00,  7.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513      0.613       0.48      0.522      0.258\n",
      "                   Gun        593        932      0.794      0.847      0.884      0.476\n",
      "                 Knife        407        622       0.49      0.579      0.537      0.256\n",
      "                Wrench        425        614      0.597      0.327      0.395      0.192\n",
      "                Pliers        803       1109      0.778      0.234       0.45      0.217\n",
      "              Scissors        204        236      0.407      0.411      0.341      0.146\n",
      "Speed: 0.2ms preprocess, 244.4ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/yolo11/phase1\u001b[0m\n",
      "Phase 2: unfreezing all layers, fine-tuning entire model...\n",
      "New https://pypi.org/project/ultralytics/8.3.119 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.105 🚀 Python-3.10.16 torch-2.4.1.post2 CPU (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11l.pt, data=/Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/data.yaml, epochs=4, time=None, patience=100, batch=8, imgsz=416, save=True, save_period=-1, cache=False, device=cpu, workers=0, project=runs/yolo11, name=phase2, exist_ok=True, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/yolo11/phase2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1415650  ultralytics.nn.modules.head.Detect           [6, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,315,106 parameters, 25,315,090 gradients\n",
      "\n",
      "Transferred 1015/1015 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/yolo11/phase2', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage... 14122 images, 7078 backgrounds, 0 corrupt: 100%|██████████| 14122/14122 [00:01<00:00, 8123.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage... 3532 images, 1770 backgrounds, 0 corrupt: 100%|██████████| 3532/3532 [00:00<00:00, 7904.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage.cache\n",
      "Plotting labels to runs/yolo11/phase2/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0001, momentum=0.937) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/yolo11/phase2\u001b[0m\n",
      "Starting training for 4 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/4         0G      1.687      1.624      1.704          0        416: 100%|██████████| 1766/1766 [5:30:23<00:00, 11.23s/it]      \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 221/221 [57:04<00:00, 15.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513       0.68      0.561      0.645      0.362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/4         0G      1.583      1.473      1.636          3        416: 100%|██████████| 1766/1766 [1:20:56<00:00,  2.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 221/221 [17:34<00:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513      0.713      0.612       0.69      0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/4         0G      1.534      1.386      1.592         12        416: 100%|██████████| 1766/1766 [1:22:21<00:00,  2.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 221/221 [17:37<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513       0.72      0.637      0.715      0.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/4         0G      1.479      1.321      1.553          4        416: 100%|██████████| 1766/1766 [1:35:17<00:00,  3.24s/it]    \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 221/221 [30:15<00:00,  8.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513      0.759      0.649      0.737      0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 epochs completed in 11.862 hours.\n",
      "Optimizer stripped from runs/yolo11/phase2/weights/last.pt, 51.2MB\n",
      "Optimizer stripped from runs/yolo11/phase2/weights/best.pt, 51.2MB\n",
      "\n",
      "Validating runs/yolo11/phase2/weights/best.pt...\n",
      "Ultralytics 8.3.105 🚀 Python-3.10.16 torch-2.4.1.post2 CPU (Apple M4)\n",
      "YOLO11l summary (fused): 190 layers, 25,283,938 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 221/221 [22:51<00:00,  6.21s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513      0.759       0.65      0.737      0.455\n",
      "                   Gun        593        932      0.871      0.942      0.969      0.651\n",
      "                 Knife        407        622      0.862       0.63      0.781      0.473\n",
      "                Wrench        425        614      0.655      0.539      0.621      0.401\n",
      "                Pliers        803       1109      0.743      0.609      0.716      0.421\n",
      "              Scissors        204        236      0.666       0.53        0.6      0.328\n",
      "Speed: 0.2ms preprocess, 294.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/yolo11/phase2\u001b[0m\n",
      "Training complete. Models and logs are in runs/yolo11/phase{1,2}/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Converts VOC XML → YOLO .txt\n",
    "2) Writes data.yaml\n",
    "3) Trains YOLOv11 in two phases:\n",
    "     • Phase 1: freeze backbone, head only\n",
    "     • Phase 2: unfreeze all, fine-tune\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "\n",
    "# 1. Install ultralytics if needed:\n",
    "#    pip install ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "BASE_DIR        = \"/Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data\"\n",
    "IMAGES_DIR      = os.path.join(BASE_DIR, \"JPEGImageFull\", \"dataset\", \"JPEGImage\")\n",
    "ANNOTATIONS_DIR = os.path.join(BASE_DIR, \"positive-Annotation\")\n",
    "LABELS_DIR      = os.path.join(BASE_DIR, \"labels\")  # where .txt goes\n",
    "DATA_YAML       = os.path.join(BASE_DIR, \"data.yaml\")\n",
    "\n",
    "# Class ↔ ID mapping\n",
    "CLASS_MAP = {\n",
    "    \"Gun\": 0, \"Knife\": 1, \"Wrench\": 2,\n",
    "    \"Pliers\": 3, \"Scissors\": 4, \"Hammer\": 5\n",
    "}\n",
    "\n",
    "os.makedirs(LABELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ─── STEP 1: XML → YOLO .TXT (with guards) ────────────────────────────────────\n",
    "\n",
    "print(\"Converting XML annotations to YOLO .txt format (skipping malformed objects)...\")\n",
    "for xml_path in glob.glob(os.path.join(ANNOTATIONS_DIR, \"*.xml\")):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find(\"size\")\n",
    "    w = float(size.find(\"width\").text)\n",
    "    h = float(size.find(\"height\").text)\n",
    "\n",
    "    label_path = os.path.join(\n",
    "        LABELS_DIR,\n",
    "        os.path.basename(xml_path).replace(\".xml\", \".txt\")\n",
    "    )\n",
    "\n",
    "    with open(label_path, \"w\") as f:\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name_node = obj.find(\"name\")\n",
    "            if name_node is None or name_node.text not in CLASS_MAP:\n",
    "                # skip objects without a valid class\n",
    "                continue\n",
    "            cls_name = name_node.text\n",
    "            cid = CLASS_MAP[cls_name]\n",
    "\n",
    "            bnd = obj.find(\"bndbox\")\n",
    "            if bnd is None:\n",
    "                # skip objects without a bounding-box\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                xmin = float(bnd.find(\"xmin\").text)\n",
    "                ymin = float(bnd.find(\"ymin\").text)\n",
    "                xmax = float(bnd.find(\"xmax\").text)\n",
    "                ymax = float(bnd.find(\"ymax\").text)\n",
    "            except (AttributeError, TypeError):\n",
    "                # if any coordinate is missing, skip this object\n",
    "                continue\n",
    "\n",
    "            # Normalize to YOLO format\n",
    "            x_center = ((xmin + xmax) / 2) / w\n",
    "            y_center = ((ymin + ymax) / 2) / h\n",
    "            bw = (xmax - xmin) / w\n",
    "            bh = (ymax - ymin) / h\n",
    "\n",
    "            f.write(f\"{cid} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ─── STEP 2: CSV SPLITS + GUARANTEE .TXT + WRITE data.yaml ────────────────────\n",
    "print(\"Generating splits, creating missing .txt labels, and writing data.yaml…\")\n",
    "\n",
    "# 1) Paths to your CSVs\n",
    "TRAIN_CSV = os.path.join(BASE_DIR, \"TestTrainSplits\", \"train_test_easy\", \"train-3000.csv\")\n",
    "VAL_CSV   = os.path.join(BASE_DIR, \"TestTrainSplits\", \"train_test_easy\", \"test-3000.csv\")\n",
    "\n",
    "# 2) Read the image IDs\n",
    "train_ids = pd.read_csv(TRAIN_CSV)['name'].tolist()\n",
    "val_ids   = pd.read_csv(VAL_CSV)['name'].tolist()\n",
    "\n",
    "# 3) Write train.txt & val.txt (absolute paths to .jpg)\n",
    "train_list = os.path.join(BASE_DIR, \"train.txt\")\n",
    "val_list   = os.path.join(BASE_DIR, \"val.txt\")\n",
    "\n",
    "with open(train_list, 'w') as f:\n",
    "    for img_id in train_ids:\n",
    "        f.write(os.path.join(IMAGES_DIR, f\"{img_id}.jpg\") + \"\\n\")\n",
    "\n",
    "with open(val_list, 'w') as f:\n",
    "    for img_id in val_ids:\n",
    "        f.write(os.path.join(IMAGES_DIR, f\"{img_id}.jpg\") + \"\\n\")\n",
    "\n",
    "# 4) Ensure every split image has *some* .txt label (empty if no objects)\n",
    "for img_id in set(train_ids + val_ids):\n",
    "    lbl_path = os.path.join(LABELS_DIR, f\"{img_id}.txt\")\n",
    "    if not os.path.isfile(lbl_path):\n",
    "        open(lbl_path, 'w').close()\n",
    "\n",
    "# 5) Write data.yaml with an explicit path:\n",
    "print(\"Writing data.yaml…\")\n",
    "# ─── WRITE data.yaml ─────────────────────────────────────────────────────────\n",
    "data_yaml = f\"\"\"\\\n",
    "train: {train_list}\n",
    "val:   {val_list}\n",
    "nc:    {len(CLASS_MAP)}\n",
    "names: {list(CLASS_MAP.keys())}\n",
    "\"\"\"\n",
    "with open(DATA_YAML, \"w\") as f:\n",
    "    f.write(data_yaml)\n",
    "print(\"Wrote data.yaml without a `path:`. YOLOv11 will now use your split files directly.\")\n",
    "\n",
    "\n",
    "print(\"STEP 2 complete.\")\n",
    "\n",
    "\n",
    "# ─── STEP 3: INITIALIZE & TRAIN YOLOv11 ────────────────────────────────────────\n",
    "\n",
    "# Choose your variant: nano / small / medium / large / xlarge\n",
    "PRETRAINED = \"yolo11l.pt\"\n",
    "\n",
    "print(f\"Loading YOLOv11 model ({PRETRAINED})...\")\n",
    "model = YOLO(PRETRAINED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Phase 1: freeze backbone → train head only\n",
    "print(\"Phase 1: freezing backbone, training head...\")\n",
    "model.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=2,\n",
    "    imgsz=416,\n",
    "    batch=16,\n",
    "    lr0=1e-3,\n",
    "    optimizer=\"AdamW\",\n",
    "    freeze=10,          # freeze first 10 layers of backbone\n",
    "    device=device,\n",
    "    project=\"runs/yolo11\",\n",
    "    name=\"phase1\",\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Phase 2: unfreeze all → fine-tune end-to-end\n",
    "print(\"Phase 2: unfreezing all layers, fine-tuning entire model...\")\n",
    "model.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=4,\n",
    "    imgsz=416,\n",
    "    batch=8,\n",
    "    lr0=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    device=device,\n",
    "    project=\"runs/yolo11\",\n",
    "    name=\"phase2\",\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "print(\"Training complete. Models and logs are in runs/yolo11/phase{1,2}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af432e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66174b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.105 🚀 Python-3.10.16 torch-2.4.1.post2 CPU (Apple M4)\n",
      "YOLO11l summary (fused): 190 layers, 25,283,938 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/JPEGImageFull/dataset/JPEGImage.cache... 3532 images, 1770 backgrounds, 0 corrupt: 100%|██████████| 3532/3532 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 221/221 [17:12<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3532       3513      0.759       0.65      0.737      0.455\n",
      "                   Gun        593        932      0.871      0.942      0.969      0.651\n",
      "                 Knife        407        622      0.862       0.63      0.781      0.473\n",
      "                Wrench        425        614      0.655      0.539      0.621      0.401\n",
      "                Pliers        803       1109      0.743      0.609      0.716      0.421\n",
      "              Scissors        204        236      0.666       0.53        0.6      0.328\n",
      "Speed: 0.1ms preprocess, 289.9ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
      "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0, 1, 2, 3, 4])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x35f7a10f0>\n",
      "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
      "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,    0.061902,    0.030951,           0],\n",
      "       [          1,           1,           1, ...,   0.0032903,   0.0016452,           0],\n",
      "       [          1,           1,           1, ...,  0.00087752,  0.00043876,           0],\n",
      "       [          1,           1,           1, ...,   0.0033943,   0.0016971,           0],\n",
      "       [          1,           1,           1, ...,  0.00061123,  0.00030562,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.2342,     0.23423,      0.3072, ...,           0,           0,           0],\n",
      "       [    0.10495,     0.10504,     0.14877, ...,           0,           0,           0],\n",
      "       [   0.043252,    0.043279,    0.059123, ...,           0,           0,           0],\n",
      "       [    0.07073,    0.070769,    0.095548, ...,           0,           0,           0],\n",
      "       [   0.028043,    0.028053,    0.045432, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.1327,     0.13273,     0.18165, ...,           1,           1,           1],\n",
      "       [   0.055489,    0.055536,    0.080691, ...,           1,           1,           1],\n",
      "       [    0.02213,    0.022144,    0.030518, ...,           1,           1,           1],\n",
      "       [   0.036691,    0.036712,    0.050231, ...,           1,           1,           1],\n",
      "       [   0.014231,    0.014236,    0.023281, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.99571,     0.99571,     0.99464, ...,           0,           0,           0],\n",
      "       [    0.96624,     0.96624,     0.95177, ...,           0,           0,           0],\n",
      "       [    0.94951,     0.94951,       0.943, ...,           0,           0,           0],\n",
      "       [    0.97836,     0.97836,     0.97656, ...,           0,           0,           0],\n",
      "       [    0.95339,     0.95339,     0.93644, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
      "fitness: 0.48321067752756525\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
      "maps: array([    0.65109,     0.47341,     0.40107,     0.42115,     0.32814,     0.45497])\n",
      "names: {0: 'Gun', 1: 'Knife', 2: 'Wrench', 3: 'Pliers', 4: 'Scissors', 5: 'Hammer'}\n",
      "plot: True\n",
      "results_dict: {'metrics/precision(B)': 0.7592637084663627, 'metrics/recall(B)': 0.6498773295175383, 'metrics/mAP50(B)': 0.7373562903033515, 'metrics/mAP50-95(B)': 0.4549722761080334, 'fitness': 0.48321067752756525}\n",
      "save_dir: PosixPath('runs/detect/val')\n",
      "speed: {'preprocess': 0.14738615908520772, 'inference': 289.88076750685974, 'loss': 6.21126171007398e-05, 'postprocess': 0.12890278390215315}\n",
      "task: 'detect'\n"
     ]
    }
   ],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# load your best Phase 2 weights\n",
    "model = YOLO(\"runs/yolo11/phase2/weights/best.pt\")\n",
    "\n",
    "# run validation on your splits\n",
    "# this re-runs the same evaluation you saw at train-time\n",
    "metrics = model.val(\n",
    "    data=\"/Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data/data.yaml\",\n",
    "    imgsz=416,\n",
    "    batch=16\n",
    ")\n",
    "\n",
    "print(metrics)  # dict with P, R, mAP50, mAP50-95 per class + overall\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
