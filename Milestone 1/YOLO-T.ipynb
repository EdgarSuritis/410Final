{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bf421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb85483",
   "metadata": {},
   "source": [
    "In this part, we are creating a YOLO-T model. The model uses a Swin Transformer as the backbone (loaded from the timm library) and a custom YOLO detection head that fuses features from three scales. The detection head mimics the YOLOv3 design for multi-scale predictions.\n",
    "\n",
    "---\n",
    "\n",
    "Ways we can try and improve from this model:\n",
    "- Use the latest YOLO model.\n",
    "- Add the EAOD-Net modifications.\n",
    "- Try and use the information gained from the extraction with another model: like neural-net, random forest, and other applicable ones.\n",
    "- Balance between precision and accuracy. [Training a model that has high accuracy, a model that has high precision, and then putting the results from both of those together.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b74d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import get_dataloader, custom_collate_fn\n",
    "\n",
    "# =============================================================================\n",
    "# Adjust these paths according to your folder structure.\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # For the train_test_easy split\n",
    "    base_dir = \"/Users/jamesngugi/Desktop/Applied ML/ML-Project/test-data\"\n",
    "    \n",
    "    # Use the CSV files in the easy split folder:\n",
    "    csv_train = os.path.join(base_dir, \"TestTrainSplits\", \"train_test_easy\", \"train-3000.csv\")\n",
    "    csv_test  = os.path.join(base_dir, \"TestTrainSplits\", \"train_test_easy\", \"test-3000.csv\")\n",
    "    \n",
    "    # Directory containing JPEG images.\n",
    "    images_dir = os.path.join(base_dir, \"JPEGImageFull\", \"dataset\", \"JPEGImage\")\n",
    "    # Directory containing positive XML annotations.\n",
    "    annotations_dir = os.path.join(base_dir, \"positive-Annotation\")\n",
    "    \n",
    "    # DataLoaders for training and testing.\n",
    "    # Pass the custom collate function here:\n",
    "    train_loader = get_dataloader(csv_train, images_dir, annotations_dir, batch_size=32, train=True)\n",
    "    test_loader  = get_dataloader(csv_test, images_dir, annotations_dir, batch_size=32, train=False)\n",
    "    \n",
    "    # When creating the DataLoader inside get_dataloader, set the collate_fn parameter\n",
    "    \n",
    "    # For our testing, we can either modify get_dataloader() or wrap it here:\n",
    "    from torch.utils.data import DataLoader\n",
    "    # Reconstruct using our custom_collate_fn for demonstration:\n",
    "    train_loader = DataLoader(train_loader.dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    # Simple test: iterate through one batch.\n",
    "    for imgs, targets in train_loader:\n",
    "        print(\"Train Images shape:\", imgs.shape)  # Expected: [batch, 3, 416, 416]\n",
    "        print(\"Train Targets:\", targets)  # A list, each element a tensor of shape [N, 4] (or [N, 5] if you include classes)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e10aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/2jz1j73d6kl_wvvcg3ld5d_00000gn/T/ipykernel_1769/3245617342.py:287: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/jamesngugi/miniforge3/envs/ml/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n",
      "Avg IoU:   0.0000\n",
      "TP: 0  FP: 13  FN: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Make sure you have imported your custom_collate_fn from your datasets module\n",
    "# from datasets import custom_collate_fn\n",
    "\n",
    "# Rebuild your test loader with the custom collate function.\n",
    "test_loader = DataLoader(\n",
    "    test_loader.dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Utility Functions for Evaluation\n",
    "# ---------------------------\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Computes Intersection over Union (IoU) for two boxes.\n",
    "    Boxes are in the format [x1, y1, x2, y2].\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1g, y1g, x2g, y2g = box2\n",
    "    \n",
    "    inter_x1 = max(x1, x1g)\n",
    "    inter_y1 = max(y1, y1g)\n",
    "    inter_x2 = min(x2, x2g)\n",
    "    inter_y2 = min(y2, y2g)\n",
    "    \n",
    "    inter_area = max(inter_x2 - inter_x1, 0) * max(inter_y2 - inter_y1, 0)\n",
    "    area1 = (x2 - x1) * (y2 - y1)\n",
    "    area2 = (x2g - x1g) * (y2g - y1g)\n",
    "    union_area = area1 + area2 - inter_area + 1e-6  # avoid division by zero\n",
    "    return inter_area / union_area\n",
    "\n",
    "def convert_gt_to_pixels(gt_list, img_size=416):\n",
    "    \"\"\"\n",
    "    Converts a list of ground truth boxes in YOLO format \n",
    "    [class, cx, cy, w, h] (normalized) to pixel coordinates [x1, y1, x2, y2, class].\n",
    "    \"\"\"\n",
    "    converted = []\n",
    "    for gt in gt_list:\n",
    "        cls, cx, cy, w, h = gt\n",
    "        x1 = (cx - w/2) * img_size\n",
    "        y1 = (cy - h/2) * img_size\n",
    "        x2 = (cx + w/2) * img_size\n",
    "        y2 = (cy + h/2) * img_size\n",
    "        converted.append([x1, y1, x2, y2, int(cls)])\n",
    "    return converted\n",
    "\n",
    "def evaluate_detections(all_preds, all_gts, iou_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Compares predictions and ground truths across all images.\n",
    "    For each image, a prediction is considered a true positive (TP) if it matches a ground truth (GT)\n",
    "    with the same class and IoU >= iou_threshold. Otherwise, it is a false positive (FP).\n",
    "    Ground truths with no matching prediction are counted as false negatives (FN).\n",
    "    Returns a dictionary of overall metrics.\n",
    "    \"\"\"\n",
    "    total_TP = 0\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "    total_iou = 0.0\n",
    "    iou_count = 0\n",
    "    \n",
    "    # Loop over each image.\n",
    "    for preds, gt in zip(all_preds, all_gts):\n",
    "        # Convert ground truths to pixel coordinates.\n",
    "        gts_pixels = convert_gt_to_pixels(gt, img_size=416)\n",
    "        matched_gts = set()  # to keep track of ground truths that are already matched\n",
    "        \n",
    "        # Process each prediction.\n",
    "        for pred in preds:\n",
    "            # pred is [x1, y1, x2, y2, conf, cls] with pixel coordinates\n",
    "            x1p, y1p, x2p, y2p, conf, cls_pred = pred\n",
    "            best_iou = 0.0\n",
    "            best_gt_idx = -1\n",
    "            for idx, gt_box in enumerate(gts_pixels):\n",
    "                # Only consider ground truths of the same class.\n",
    "                if gt_box[4] != int(cls_pred):\n",
    "                    continue\n",
    "                iou_val = compute_iou(pred[:4], gt_box[:4])\n",
    "                if iou_val > best_iou:\n",
    "                    best_iou = iou_val\n",
    "                    best_gt_idx = idx\n",
    "            \n",
    "            # A valid match is found if IoU is above threshold and the GT hasn't been matched.\n",
    "            if best_iou >= iou_threshold and best_gt_idx not in matched_gts:\n",
    "                total_TP += 1\n",
    "                total_iou += best_iou\n",
    "                iou_count += 1\n",
    "                matched_gts.add(best_gt_idx)\n",
    "            else:\n",
    "                total_FP += 1\n",
    "        \n",
    "        # All GTs not matched are false negatives.\n",
    "        total_FN += len(gts_pixels) - len(matched_gts)\n",
    "    \n",
    "    precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "    recall    = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "    f1_score  = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    avg_iou   = total_iou / iou_count if iou_count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"avg_iou\": avg_iou,\n",
    "        \"true_positives\": total_TP,\n",
    "        \"false_positives\": total_FP,\n",
    "        \"false_negatives\": total_FN,\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Modified decode_predictions Function (with debug prints removed)\n",
    "# ---------------------------\n",
    "def decode_predictions(preds_tuple, conf_thresh=0.25):\n",
    "    \"\"\"\n",
    "    Decodes predictions from bbox, objectness, and class tensors (for simple model).\n",
    "    Returns a list of detections for one image.\n",
    "    Each detection is [x1, y1, x2, y2, conf, cls].\n",
    "    \"\"\"\n",
    "    bbox_pred, objectness_pred, class_pred = preds_tuple # Unpack the tuple of tensors\n",
    "    bbox_pred = bbox_pred.detach().cpu() # [1, H, W, 4] - assuming batch size 1 passed here\n",
    "    objectness_pred = objectness_pred.detach().cpu() # [1, H, W, 1]\n",
    "    class_pred = class_pred.detach().cpu() # [1, H, W, num_classes]\n",
    "\n",
    "    if bbox_pred.ndim != 4: # Check expected ndim for bbox\n",
    "        raise ValueError(f\"Expected bbox_pred to have 4 dimensions (B, H, W, 4), got {bbox_pred.ndim}\")\n",
    "    B, grid_h, grid_w, _ = bbox_pred.shape # Get grid size from bbox_pred\n",
    "    if grid_h != grid_w:\n",
    "        raise ValueError(f\"Expected square grid but got {grid_h} and {grid_w}\")\n",
    "    grid_size = grid_h\n",
    "    num_classes = class_pred.shape[-1] # Get num_classes from class_pred\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    # Create grids for offset calculation. (Same as before)\n",
    "    grid_x = torch.arange(grid_size).repeat(grid_size, 1).view(1, grid_size, grid_size).float()\n",
    "    grid_y = torch.arange(grid_size).repeat(grid_size, 1).t().view(1, grid_size, grid_size).float()\n",
    "\n",
    "\n",
    "    # Process predictions (similar logic as before, but directly using the tensors)\n",
    "    box = torch.zeros_like(bbox_pred[..., :4]) # Initialize box tensor [1, H, W, 4]\n",
    "    box[..., 0] = (torch.sigmoid(bbox_pred[..., 0]) + grid_x) / grid_size # cx\n",
    "    box[..., 1] = (torch.sigmoid(bbox_pred[..., 1]) + grid_y) / grid_size # cy\n",
    "\n",
    "    dummy_anchor = torch.tensor([0.5, 0.5]).view(1, 1, 1, 2).type_as(bbox_pred) # Dummy anchor\n",
    "    box[..., 2] = dummy_anchor[..., 0] * torch.exp(bbox_pred[..., 2]) # w\n",
    "    box[..., 3] = dummy_anchor[..., 1] * torch.exp(bbox_pred[..., 3]) # h\n",
    "\n",
    "    conf = torch.sigmoid(objectness_pred[..., 0]) # Objectness confidence\n",
    "    cls_prob = torch.softmax(class_pred, dim=-1) # Class probabilities\n",
    "    cls_conf, cls_pred = torch.max(cls_prob, dim=-1) # Max class prob and class index\n",
    "    final_conf = conf * cls_conf # Final confidence score\n",
    "\n",
    "\n",
    "    # Iterate through grid cells (as before)\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if final_conf[0, i, j] > conf_thresh: # Check confidence threshold\n",
    "                x_center, y_center, w, h = box[0, i, j] # Get box params\n",
    "                # Scale to original image dimensions (assumed to be 416x416).\n",
    "                x1 = (x_center - w/2) * 416\n",
    "                y1 = (y_center - h/2) * 416\n",
    "                x2 = (x_center + w/2) * 416\n",
    "                y2 = (y_center - h/2) * 416 # Typo in original: should be + h/2\n",
    "                y2 = (y_center + h/2) * 416 # Corrected line\n",
    "                detections.append([\n",
    "                    x1.item(), y1.item(), x2.item(), y2.item(),\n",
    "                    final_conf[0, i, j].item(), cls_pred[0, i, j].item()\n",
    "                ])\n",
    "    return detections\n",
    "\n",
    "# ---------------------------\n",
    "# Updated Inference Function with Metrics Collection\n",
    "# ---------------------------\n",
    "def inference(model, dataloader, device, output_dir='output', conf_thresh=0.3, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Runs inference on the given dataloader, saves the detection images, and collects results to evaluate metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cls_names = ['gun', 'knife', 'wrench', 'pliers', 'scissors', 'hammer']\n",
    "    \n",
    "    all_preds = []  # List to store predictions for each image.\n",
    "    all_gts = []    # List to store ground truth boxes for each image.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (images, targets) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            preds_scales = model(images)  # Output is now (bbox_pred, objectness_pred, class_pred)\n",
    "            # print(\"Shape of preds_scales:\", [p.shape for p in preds_scales]) # Debug print - No longer a list\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                # Prepare image for drawing.\n",
    "                img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "                img = np.ascontiguousarray(img)  # Ensure contiguous layout.\n",
    "\n",
    "                # Decode predictions - Directly pass bbox_pred, objectness_pred, class_pred\n",
    "                bbox_pred, objectness_pred, class_pred = preds_scales # Unpack the tuple\n",
    "                img_preds = (bbox_pred[i:i+1], objectness_pred[i:i+1], class_pred[i:i+1]) # Take i-th image batch from each\n",
    "                dets = decode_predictions(img_preds, conf_thresh=conf_thresh) # Modified decode_predictions to accept tuple\n",
    "                # Perform Non-Maximum Suppression.\n",
    "                dets = non_max_suppression(dets, conf_thresh=conf_thresh, iou_thresh=iou_threshold)\n",
    "                \n",
    "                # Save detections for this image.\n",
    "                all_preds.append(dets)\n",
    "                # Store ground truths. targets[i] is a tensor of shape [N, 5].\n",
    "                gt_list = targets[i].cpu().numpy().tolist()\n",
    "                all_gts.append(gt_list)\n",
    "                \n",
    "                # Draw detections on the image.\n",
    "                for det in dets:\n",
    "                    x1, y1, x2, y2, conf, cls = det\n",
    "                    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                    cv2.putText(img, f\"{cls_names[int(cls)]} {conf:.2f}\", (int(x1), int(y1)-10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                # out_path = os.path.join(output_dir, f\"result_{idx}_{i}.jpg\")\n",
    "                # cv2.imwrite(out_path, img)\n",
    "                # print(f\"Saved detection result to {out_path}\")\n",
    "    \n",
    "    # After processing all images, evaluate detections.\n",
    "    metrics = evaluate_detections(all_preds, all_gts, iou_threshold=iou_threshold)\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"Avg IoU:   {metrics['avg_iou']:.4f}\")\n",
    "    print(f\"TP: {metrics['true_positives']}  FP: {metrics['false_positives']}  FN: {metrics['false_negatives']}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Non-Maximum Suppression Function (unchanged)\n",
    "# ---------------------------\n",
    "def non_max_suppression(detections, conf_thresh=0.25, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Applies Non-Maximum Suppression (NMS) on the detections.\n",
    "    Returns the final list of detection boxes.\n",
    "    \"\"\"\n",
    "    if len(detections) == 0:\n",
    "        return []\n",
    "    detections = np.array(detections)\n",
    "    # Filter detections with confidence lower than the threshold.\n",
    "    detections = detections[detections[:, 4] >= conf_thresh]\n",
    "    if len(detections) == 0:\n",
    "        return []\n",
    "    # Sort detections by confidence (highest first).\n",
    "    indices = np.argsort(-detections[:, 4])\n",
    "    detections = detections[indices]\n",
    "    final_dets = []\n",
    "    while len(detections) > 0:\n",
    "        best = detections[0]\n",
    "        final_dets.append(best)\n",
    "        if len(detections) == 1:\n",
    "            break\n",
    "        rest = detections[1:]\n",
    "        x1 = np.maximum(best[0], rest[:, 0])\n",
    "        y1 = np.maximum(best[1], rest[:, 1])\n",
    "        x2 = np.minimum(best[2], rest[:, 2])\n",
    "        y2 = np.minimum(best[3], rest[:, 3])\n",
    "        inter_area = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n",
    "        best_area = (best[2] - best[0]) * (best[3] - best[1])\n",
    "        rest_area = (rest[:, 2] - rest[:, 0]) * (rest[:, 3] - rest[:, 1])\n",
    "        iou = inter_area / (best_area + rest_area - inter_area + 1e-6)\n",
    "        detections = rest[iou < iou_thresh]\n",
    "    return final_dets\n",
    "\n",
    "# ---------------------------\n",
    "# Main Inference Execution\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Assume model and test_loader are defined elsewhere.\n",
    "    # For example:\n",
    "    #   from model_definition import YOLOTModel\n",
    "    #   model = YOLOTModel(num_classes=6).to(device)\n",
    "    \n",
    "    checkpoint_path = 'trained_models_simple/simple_model_state_3000.pth'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        # Loading the saved checkpoint.\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        print(\"Loaded checkpoint for inference.\")\n",
    "    else:\n",
    "        print(\"Checkpoint not found. Using current model weights.\")\n",
    "    \n",
    "    # Run inference and evaluation.\n",
    "    inference(model, test_loader, device, output_dir='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define anchors for each scale (example values; normalize relative to input size 416)\n",
    "ANCHORS = {\n",
    "    'large':  [(0.10, 0.13), (0.16, 0.30), (0.33, 0.23)],  # for 52x52\n",
    "    'medium': [(0.22, 0.27), (0.38, 0.56), (0.95, 0.80)],  # for 26x26\n",
    "    'small':  [(0.90, 1.10), (1.87, 3.23), (4.42, 2.74)]   # for 13x13\n",
    "}\n",
    "# Note: These anchor values are exemplary. In a production setup, we should use k-means on SIXray boxes.\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# the images output by preprocessing must be compatible with the same format: images as tensors and targets as a tensor of shape [N, 5] where each row is [class, x_center, y_center, w, h] with all values normalized.\n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_dim=416, ignore_thresh=0.5,\n",
    "                 lambda_coord=5.0, lambda_noobj=0.5):\n",
    "        \"\"\"\n",
    "        anchors: list of (w, h) for this scale (normalized)\n",
    "        num_classes: number of classes\n",
    "        img_dim: input image dimension (assumed square)\n",
    "        ignore_thresh: IoU threshold for ignoring objectness loss in no-object cells\n",
    "        lambda_coord: weight for coordinate loss\n",
    "        lambda_noobj: weight for no-object confidence loss\n",
    "        \"\"\"\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.anchors = anchors  # for one scale\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.img_dim = img_dim\n",
    "        self.ignore_thresh = ignore_thresh\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        self.bce_loss = nn.BCELoss(reduction='sum')\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    def forward(self, prediction, targets):\n",
    "        \"\"\"\n",
    "        prediction: tensor of shape [batch, (5+num_classes)*num_anchors, grid, grid]\n",
    "        targets: list of targets for each image; each target is a tensor of shape [N, 5],\n",
    "                 with [cls, x_center, y_center, w, h] in normalized coordinates.\n",
    "        \"\"\"\n",
    "        batch_size = prediction.size(0)\n",
    "        grid_size = prediction.size(2)  # square grid\n",
    "        stride = self.img_dim / grid_size\n",
    "        \n",
    "        prediction = prediction.view(batch_size, self.num_anchors, self.num_classes + 5, grid_size, grid_size)\n",
    "        prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()  # shape: [B, A, grid, grid, 5+num_classes]\n",
    "        \n",
    "        # Get outputs\n",
    "        pred_tx = prediction[..., 0]  # center x\n",
    "        pred_ty = prediction[..., 1]  # center y\n",
    "        pred_tw = prediction[..., 2]  # width\n",
    "        pred_th = prediction[..., 3]  # height\n",
    "        pred_conf = prediction[..., 4]  # objectness\n",
    "        pred_cls = prediction[..., 5:]  # class scores\n",
    "        \n",
    "        # Create grid offsets\n",
    "        grid_x = torch.arange(grid_size).repeat(grid_size, 1).view([1, 1, grid_size, grid_size]).type_as(prediction)\n",
    "        grid_y = torch.arange(grid_size).repeat(grid_size, 1).t().view([1, 1, grid_size, grid_size]).type_as(prediction)\n",
    "        \n",
    "        # Transform predictions to bounding box coordinates\n",
    "        # According to YOLOv3: \n",
    "        # x = sigmoid(tx) + grid_x, similarly for y.\n",
    "        # w = anchor_w * exp(tw), h = anchor_h * exp(th)\n",
    "        pred_boxes = torch.zeros(prediction[..., :4].shape).type_as(prediction)\n",
    "        pred_boxes[..., 0] = (sigmoid(pred_tx) + grid_x) / grid_size\n",
    "        pred_boxes[..., 1] = (sigmoid(pred_ty) + grid_y) / grid_size\n",
    "        # Prepare anchors tensor\n",
    "        anchors_tensor = torch.tensor(self.anchors).type_as(prediction)  # shape: [num_anchors, 2]\n",
    "        anchors_tensor = anchors_tensor.view(1, self.num_anchors, 1, 1, 2)\n",
    "        pred_boxes[..., 2] = anchors_tensor[..., 0] * torch.exp(pred_tw)\n",
    "        pred_boxes[..., 3] = anchors_tensor[..., 1] * torch.exp(pred_th)\n",
    "        \n",
    "        # Convert targets to tensor for matching.\n",
    "        # For each image, create a target tensor of shape [batch, num_anchors, grid, grid, 5+num_classes]\n",
    "        target_tensor = torch.zeros_like(prediction)\n",
    "        # Also create object mask.\n",
    "        obj_mask = torch.zeros(batch_size, self.num_anchors, grid_size, grid_size).type_as(prediction)\n",
    "        noobj_mask = torch.ones(batch_size, self.num_anchors, grid_size, grid_size).type_as(prediction)\n",
    "        class_mask = torch.zeros(batch_size, self.num_anchors, grid_size, grid_size).type_as(prediction)\n",
    "        t_box = torch.zeros_like(pred_boxes)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            if targets[b].nelement() == 0:\n",
    "                continue\n",
    "            for target in targets[b]:\n",
    "                # target: [cls, x, y, w, h]\n",
    "                cls = target[0]\n",
    "                x, y, w, h = target[1], target[2], target[3], target[4]\n",
    "                i = int(x * grid_size)\n",
    "                j = int(y * grid_size)\n",
    "                # Find best anchor based on IoU between target and anchors (ignoring grid cell offset)\n",
    "                gt_box = torch.tensor([0, 0, w, h]).unsqueeze(0)  # center not needed here\n",
    "                anchor_shapes = torch.cat([torch.zeros((self.num_anchors,2)), anchors_tensor[0, :, 0, 0, :]], dim=1)\n",
    "                # Compute IoU between gt_box and each anchor box\n",
    "                inter = torch.min(gt_box[:,2:], anchor_shapes[:,2:]).prod(1)\n",
    "                union = (gt_box[:,2:]*torch.ones_like(anchor_shapes[:,2:])).prod(1) + anchor_shapes[:,2:].prod(1) - inter\n",
    "                ious = inter / (union + 1e-6)\n",
    "                best_anchor = torch.argmax(ious)\n",
    "                \n",
    "                # Assign ground truth to this grid cell and anchor\n",
    "                obj_mask[b, best_anchor, j, i] = 1\n",
    "                noobj_mask[b, best_anchor, j, i] = 0\n",
    "                target_tensor[b, best_anchor, j, i, 0] = sigmoid(x * grid_size - i)  # target tx\n",
    "                target_tensor[b, best_anchor, j, i, 1] = sigmoid(y * grid_size - j)  # target ty\n",
    "                target_tensor[b, best_anchor, j, i, 2] = math.log(w / (self.anchors[best_anchor][0] + 1e-6) + 1e-6)\n",
    "                target_tensor[b, best_anchor, j, i, 3] = math.log(h / (self.anchors[best_anchor][1] + 1e-6) + 1e-6)\n",
    "                target_tensor[b, best_anchor, j, i, 4] = 1  # object exists\n",
    "                # Class one-hot encoding\n",
    "                target_tensor[b, best_anchor, j, i, 5 + int(cls)] = 1\n",
    "        \n",
    "        # Losses:\n",
    "        # Localization loss (for x, y, w, h)\n",
    "        loss_x = self.mse_loss(sigmoid(pred_tx) * obj_mask, target_tensor[...,0] * obj_mask)\n",
    "        loss_y = self.mse_loss(sigmoid(pred_ty) * obj_mask, target_tensor[...,1] * obj_mask)\n",
    "        loss_w = self.mse_loss(torch.sqrt(torch.abs(pred_boxes[...,2] + 1e-6)) * obj_mask,\n",
    "                               torch.sqrt(torch.abs(torch.exp(target_tensor[...,2])) * obj_mask))\n",
    "        loss_h = self.mse_loss(torch.sqrt(torch.abs(pred_boxes[...,3] + 1e-6)) * obj_mask,\n",
    "                               torch.sqrt(torch.abs(torch.exp(target_tensor[...,3])) * obj_mask))\n",
    "        loss_coord = self.lambda_coord * (loss_x + loss_y + loss_w + loss_h)\n",
    "        \n",
    "        # Confidence loss:\n",
    "        loss_conf_obj = self.bce_loss(sigmoid(pred_conf) * obj_mask, target_tensor[...,4] * obj_mask)\n",
    "        loss_conf_noobj = self.lambda_noobj * self.bce_loss(sigmoid(pred_conf) * noobj_mask, \n",
    "                                                            target_tensor[...,4] * noobj_mask)\n",
    "        loss_conf = loss_conf_obj + loss_conf_noobj\n",
    "        \n",
    "        # Classification loss:\n",
    "        # For each cell with an object, use cross entropy loss. Reshape predictions.\n",
    "        pred_cls = pred_cls[obj_mask.bool()]\n",
    "        target_cls = target_tensor[..., 5:][obj_mask.bool()]\n",
    "        # target_cls is one-hot; get the index.\n",
    "        if pred_cls.nelement() > 0:\n",
    "            target_cls_index = torch.argmax(target_cls, dim=-1)\n",
    "            loss_cls = self.ce_loss(pred_cls, target_cls_index)\n",
    "        else:\n",
    "            loss_cls = torch.tensor(0.0).type_as(prediction)\n",
    "        \n",
    "        total_loss = loss_coord + loss_conf + loss_cls\n",
    "        return total_loss\n",
    "\n",
    "# Define the YOLO Head and YOLO-T Model.\n",
    "class YOLOHead(nn.Module):\n",
    "    def __init__(self, num_classes=6, in_channels=[192, 384, 768]):\n",
    "        super(YOLOHead, self).__init__()\n",
    "        self.conv_small = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[2], 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, (num_classes + 5) * 3, kernel_size=1)\n",
    "        )\n",
    "        self.conv_medium_upsample = nn.Conv2d(in_channels[2], 128, kernel_size=1)\n",
    "        self.conv_medium = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[1] + 128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, (num_classes + 5) * 3, kernel_size=1)\n",
    "        )\n",
    "        self.conv_large_upsample = nn.Conv2d(256, 64, kernel_size=1)\n",
    "        self.conv_large = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[0] + 64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(128, (num_classes + 5) * 3, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, feats):\n",
    "        large, medium, small = feats[0], feats[1], feats[2]\n",
    "        pred_small = self.conv_small(small)\n",
    "        up_small = nn.functional.interpolate(small, scale_factor=2, mode='nearest')\n",
    "        up_small = self.conv_medium_upsample(up_small)\n",
    "        fused_medium = torch.cat([up_small, medium], dim=1)\n",
    "        pred_medium = self.conv_medium(fused_medium)\n",
    "        up_medium = nn.functional.interpolate(fused_medium, scale_factor=2, mode='nearest')\n",
    "        up_medium = self.conv_large_upsample(up_medium)\n",
    "        fused_large = torch.cat([up_medium, large], dim=1)\n",
    "        pred_large = self.conv_large(fused_large)\n",
    "        return [pred_large, pred_medium, pred_small]\n",
    "\n",
    "class YOLOTModel(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(YOLOTModel, self).__init__()\n",
    "        self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True,\n",
    "                                          features_only=True, out_indices=(1, 2, 3))\n",
    "        self.head = YOLOHead(num_classes=num_classes, in_channels=[192, 384, 768])\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        preds = self.head(feats)\n",
    "        return preds\n",
    "\n",
    "# Instantiate model and loss functions for each scale.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = YOLOTModel(num_classes=6).to(device)\n",
    "\n",
    "# Create one YOLOLoss instance per scale using corresponding anchors.\n",
    "loss_large = YOLOLoss(ANCHORS['large'], num_classes=6, img_dim=416, ignore_thresh=0.5, \n",
    "                        lambda_coord=5.0, lambda_noobj=0.5)\n",
    "loss_medium = YOLOLoss(ANCHORS['medium'], num_classes=6, img_dim=416, ignore_thresh=0.5, \n",
    "                         lambda_coord=5.0, lambda_noobj=0.5)\n",
    "loss_small = YOLOLoss(ANCHORS['small'], num_classes=6, img_dim=416, ignore_thresh=0.5, \n",
    "                        lambda_coord=5.0, lambda_noobj=0.5)\n",
    "\n",
    "# Training loop that combines losses from all scales.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "def train_model(model, dataloader, optimizer, device, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Get predictions from model; each element in preds is for one scale.\n",
    "            preds = model(images)  # list of three tensors\n",
    "            # Compute loss for each scale\n",
    "            loss_l = loss_large(preds[0], targets)\n",
    "            loss_m = loss_medium(preds[1], targets)\n",
    "            loss_s = loss_small(preds[2], targets)\n",
    "            loss = loss_l + loss_m + loss_s\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} Loss: {loss.item():.4f}\")\n",
    "        avg_loss = total_loss_epoch / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f'yolot_epoch_{epoch+1}.pth')\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "train_model(model, train_loader, optimizer, device, num_epochs=50) # train_loader is the dataloader with the train dataset\n",
    "\n",
    "# bounding boxes, the labels of the dangerous goods identified and other data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e8bde",
   "metadata": {},
   "source": [
    "### Documentation of code and summary of what is needed for each stage in relation to my code.\n",
    "### Requirements From the Data Preprocessing Stage\n",
    "\n",
    "1. **Dataset Format:**\n",
    "   - **Images:**  \n",
    "     - Images should be read from a directory (e.g., `data/sixray/images/train` for training and a corresponding test folder).  \n",
    "     - They are assumed to be grayscale X‑ray images. (Your partner can either store them as grayscale or RGB; if grayscale, the code will convert to 3‑channel RGB.)\n",
    "   - **Annotations:**  \n",
    "     - Each image must have a corresponding text file (with the same base filename) in a dedicated labels directory (e.g., `data/sixray/labels/train`).  \n",
    "     - The label file must be in **YOLO format** – each line should be:  \n",
    "       ```\n",
    "       class_id x_center y_center width height\n",
    "       ```  \n",
    "       where all the coordinates are normalized (i.e. in the [0, 1] range).\n",
    "   - **DataLoader Compatibility:**  \n",
    "     - The preprocessing code should output a PyTorch DataLoader where each sample is a tuple:  \n",
    "       - **Image:** a tensor of shape `[3, 416, 416]` (i.e., resized to 416×416, normalized to [0,1]).  \n",
    "       - **Targets:** a tensor of shape `[N, 5]` per image (each row is `[class, x_center, y_center, w, h]` in normalized format).  \n",
    "     - If no ground truth exists for an image, targets should be an empty tensor.\n",
    "\n",
    "2. **Augmentations:**  \n",
    "   - The partner’s preprocessing should apply augmentations such as resizing (to 416×416), horizontal flipping, brightness adjustments, etc.\n",
    "   - The augmentation process must also appropriately transform the bounding boxes in YOLO format.\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Output from the Testing Stage\n",
    "\n",
    "1. **Inference Output:**  \n",
    "   - For each test image, the code will generate a list of detections.  \n",
    "   - Each detection is a bounding box defined as `[x1, y1, x2, y2, confidence, class_id]`, where coordinates are in pixel space relative to the original (or resized) image.\n",
    "\n",
    "2. **Visualization:**  \n",
    "   - The testing code will produce images with drawn bounding boxes (with labels and confidence scores) and save them to an output folder (e.g., `output/`).\n",
    "\n",
    "3. **Metrics Compatibility:**  \n",
    "   - The detection outputs (the list of boxes) should be in a standard format so they can later be used to compute evaluation metrics such as mAP externally or in a subsequent evaluation stage.\n",
    "\n",
    "---\n",
    "\n",
    "### How My Code Works and Integrates with the Preprocessing and Testing Stages\n",
    "\n",
    "1. **Input from Preprocessing Stage:**\n",
    "   - The training pipeline receives a DataLoader from the preprocessing stage.  \n",
    "   - Each batch consists of images (tensors of shape `[B, 3, 416, 416]`) and targets (a list of tensors, with each target tensor of shape `[N, 5]` for that image).\n",
    "   - The images and annotations are standardized (normalized and augmented) so they can be fed into the model.\n",
    "\n",
    "2. **Training Flow:**\n",
    "   - The YOLO-T model first passes the input image batch through the Swin Transformer backbone, which produces multi-scale feature maps.\n",
    "   - These features go into the custom YOLO head, which fuses different scales and outputs three prediction tensors, each corresponding to a different grid size (large, medium, and small scales).\n",
    "   - The prediction tensors are then processed by a complete YOLO loss function. This loss:\n",
    "     - Reshapes the predictions, applies sigmoid and exponential functions, and converts them into bounding box coordinates.\n",
    "     - Assigns ground truth targets to specific grid cells and anchors.\n",
    "     - Computes coordinate, objectness, and classification losses that are combined as the final loss.\n",
    "   - The optimizer then updates the model weights based on this loss.\n",
    "\n",
    "3. **Testing/Inference Flow:**\n",
    "   - During inference, the same model (with the learned weights) takes a test image through the backbone and head.\n",
    "   - The raw predictions are then decoded in the testing code. Decoding consists of:\n",
    "     - Rearranging the output tensor, applying sigmoid to center and objectness predictions, and exponentials for width/height.\n",
    "     - Converting these normalized values into absolute pixel coordinates.\n",
    "   - The code applies Non-Maximum Suppression (NMS) to prune overlapping detections, ensuring that each object is represented by a single bounding box.\n",
    "   - Finally, the resulting boxes (with their confidence scores and predicted class IDs) are drawn on the test image and saved. These outputs are standardized so they can later be used for metric evaluation.\n",
    "\n",
    "4. **Output for Next Stage:**\n",
    "   - For testing, the produced images (with drawn bounding boxes) and the detection results (lists of boxes) serve as input for the next code module that might compute evaluation metrics (such as mAP) or for further post-processing.\n",
    "   - Consistency is maintained because the same coordinate conversion and NMS procedure are used both during evaluation and (if needed) in subsequent visualization or metric calculation stages.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
